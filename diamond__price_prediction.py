# -*- coding: utf-8 -*-
"""Diamond _Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AgM7PyVE3T_hw5Y6ziuzDjOZnwr6jgTL
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.pylab as pylab
from sklearn.preprocessing import OneHotEncoder , LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.neighbors import KNeighborsRegressor
from sklearn import metrics
from xgboost import XGBRegressor

data = pd.read_csv("/content/diamonds.csv")
data.head()

"""**Loading Data**


This classic dataset contains the prices and other attributes of almost 54,000 diamonds. There are 10 attributes included in the dataset including the target ie. price.

Feature description:

price price in US dollars ($326--$18,823)This is the target column containing tags for the features.

The 4 Cs of Diamonds:-

**Carat (0.2--5.01)** The carat is the diamond’s physical weight measured in metric carats.  One carat equals 1/5 gram and is subdivided into 100 points. Carat weight is the most objective grade of the 4Cs.

**Cut (Fair, Good, Very Good, Premium, Ideal)** In determining the quality of the cut, the diamond grader evaluates the cutter’s skill in the fashioning of the diamond. The more precise the diamond is cut, the more captivating the diamond is to the eye.  

**Color, from J (worst) to D (best)** The colour of gem-quality diamonds occurs in many hues. In the range from colourless to light yellow or light brown. Colourless diamonds are the rarest. Other natural colours (blue, red, pink for example) are known as "fancy,” and their colour grading is different than from white colorless diamonds.  

**Clarity (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))** Diamonds can have internal characteristics known as inclusions or external characteristics known as blemishes. Diamonds without inclusions or blemishes are rare; however, most characteristics can only be seen with magnification

**Depth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)** The depth of the diamond is its height (in millimetres) measured from the culet (bottom tip) to the table (flat, top surface).

**table width of the top of the diamond relative to widest point (43--95)**

A diamond's table refers to the flat facet of the diamond seen when the stone is face up. The main purpose of a diamond table is to refract entering light rays and allow reflected light rays from within the diamond to meet the observer’s eye. The ideal table cut diamond will give the diamond stunning fire and brilliance.


"""

data.shape

data.info()

"""The first column is an index ("Unnamed: 0") and thus we are going to remove it.


"""

#The first column seems to be just index
data = data.drop(['Unnamed: 0'], axis=1)
data.describe()

"""Min value of "x", "y", "z" are zero this indicates that there are faulty values in data that represents dimensionless or 2-dimensional diamonds. So we need to filter out those as it clearly faulty data points.


"""

# Dropping dimensionless diamonds
data = data.drop(data[data['x']==0].index)
data = data.drop(data[data['y']==0].index)
data = data.drop(data[data['z']==0].index)
data.shape

"""We lost 20 data points by deleting the dimensionless(2-D or 1-D) diamonds.

**PAIRPLOT OF DATA**
"""

shade = ["#835656","#baa0a0","#ffc7c8","#a9a799","#65634a"]
ax = sns.pairplot(data,hue='cut', palette ='inferno')

"""**A few points to notice in these pair plots**

There are some features with datapoint that are far from the rest of the dataset which will affect the outcome of our regression model.

--> "y" and "z" have some dimensional outlies in our dataset that needs to be eliminated.

--> The "depth" should be capped but we must examine the regression line to be sure.

--> The "table" featured should be capped too.

--> Let's have a look at regression plots to get a close look at the outliers.

"""

ax = sns.regplot(x = "price",y = "y",data=data,fit_reg= True,scatter_kws = {"color":"#a9a799"},line_kws = {"color":"#835656"})
ax.set_title("Regression Line on Price vs 'Y'", color="#4e4c39")

ax = sns.regplot(x = "price",y = "z", data=data, fit_reg = True,scatter_kws={"color":"#a9a799"},line_kws={"color":"#835656"})
ax.set_title("Regression Line on Price vs 'Z'", color="#4e4c39")

ax = sns.regplot(x = "price",y = "depth", data=data, fit_reg = True,scatter_kws={"color":"#a9a799"},line_kws={"color":"#835656"})
ax.set_title("Regression Line on Price vs Depth", color="#4e4c39")

ax = sns.regplot(x = "price",y = "table", data=data, fit_reg = True,scatter_kws={"color":"#a9a799"},line_kws={"color":"#835656"})
ax.set_title("Regression Line on Price vs Table", color="#4e4c39")

"""We can clearly spot outliers in these attributes. Next up, we will remove these data points.


"""

# Dropping the outliers
data = data[(data['depth']<75)&(data['depth']>45)]
data = data[(data['table']<80)&(data['table']>40)]
data = data[(data['x']<30)]
data = data[(data['y']<30)]
data = data[(data['z']<30)&(data['z']>2)]
data.shape

"""We can clearly spot outliers in these attributes. Next up, we will remove these data points.Now that we have removed regression outliers, let us have a look at the pair plot of data in our hand.


"""

data

ax = sns.pairplot(data, hue='cut', palette=shade)

"""That's a much cleaner dataset. Next, we will deal with the categorical variables.


"""

# Get list of categorical variables
s = (data.dtypes =="object")
object_cols = list(s[s].index)
print("Categorical variables:")
print(object_cols)

"""We have three categorical variables. Let us have a look at them."""

plt.figure(figsize=(12,8))
ax = sns.violinplot(x='cut',y='price',data = data,palette=shade,scale='count')
ax.set_title('Violinplot For Cut vs Price',color='#4e4c39')
ax.set_xlabel('Cut',color='#4e4c39')
ax.set_ylabel('Price',color='#4e4c39')

plt.figure(figsize=(12,8))
shade_1=["#835656",'#b38182','#baa0a0','#ffc7c8','#d0cd85','#a9a799','#65634a']
ax = sns.violinplot(x='color',y='price',data=data, palette=shade_1, scale='count')
ax.set_title('Violinplot For Color vs Price',color='#4e4c39')
ax.set_xlabel('Color',color='#4e4c39')
ax.set_ylabel('Price',color='#4e4c39')

plt.figure(figsize=(12,8))
shade_2=["#835656",'#b38182','#baa0a0','#ffc7c8','#d0cd85','#a9a799','#65634a']
ax = sns.violinplot(x='clarity',y='price',data=data, palette=shade_2, scale='count')
ax.set_title('Violinplot For Clarity vs Price',color='#4e4c39')
ax.set_xlabel('Clarity',color='#4e4c39')
ax.set_ylabel('Price',color='#4e4c39')

#Make copy to avoid changing original data
label_data = data.copy()

#Apply label encoder to each column with categorical data
label_encoder = LabelEncoder()
for col in object_cols:
  label_data[col] = label_encoder.fit_transform(label_data[col])
label_data.head()

"""Lable encoding the data to get rid of object dtype."""

data.describe()

#Correlation matrix
cmap = sns.diverging_palette(70,20, s = 50, l = 40, n = 6, as_cmap = True)
cormat = label_data.corr()
f,ax = plt.subplots(figsize=(12,12))
sns.heatmap(cormat,cmap=cmap, annot = True)

"""**Points to notice:**

**1.** "x", "y" and "z" show a high correlation to the target column.

**2.** "depth", "cut" and "table" show low correlation. We could consider dropping but let's keep it.

**MODEL BUILDING**
"""

# Assigning the features as X and target as Y
X = label_data.drop(['price'], axis = 1)
y = label_data['price']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=7)

# Building pipeline of standard scalar and model for various regressors
pipeline_lr = Pipeline([('scalar1', StandardScaler()),
                        ('lr_classifier', LinearRegression())])
pipeline_dt = Pipeline([('scalar2', StandardScaler()),
                        ('dt_classifier', DecisionTreeRegressor())])
pipeline_rf = Pipeline([('scalar3', StandardScaler()),
                        ('rf_classifier', RandomForestRegressor())])
pipeline_kn = Pipeline([('scalar4', StandardScaler()),
                        ('rf_classifier', KNeighborsRegressor())])
pipeline_xgb = Pipeline([('scalar5', StandardScaler()),
                        ('rf_classifier', XGBRegressor())])

# List of all the pipelines
pipelines = [pipeline_lr,pipeline_dt,pipeline_rf,pipeline_kn,pipeline_xgb]

#Dictionary of pipelines and model types for ease of reference
pipe_dict = {0:'LinearRegression', 1:'DecisionTree', 2:'RandomForest', 3:'KNeighbors' , 4:'XGBRegressor'}

#Fit the pipeline
for pipe in pipelines:
  pipe.fit(X_train, y_train)

cv_results_rms = []
for i,model in enumerate(pipelines):
  cv_score = cross_val_score(model,X_train, y_train, scoring='neg_root_mean_squared_error',cv=10)
  cv_results_rms.append(cv_score)
  print("%s: %f " % (pipe_dict[i], cv_score.mean()))

"""**Testing the Model with the best score on the test set**

In the above scores, XGBClassifier appears to be the model with the best scoring on negative root mean square error. Let's test this model on a test set and evaluate it with different parameters.


"""

#Model prediction on test data
pred = pipeline_xgb.predict(X_test)

# Model Evaluation
print("R^2:",metrics.r2_score(y_test, pred))
print("Adjusted R^2:",1 - (1-metrics.r2_score(y_test, pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))
print("MAE:",metrics.mean_absolute_error(y_test, pred))
print("MSE:",metrics.mean_squared_error(y_test, pred))
print("RMSE:",np.sqrt(metrics.mean_squared_error(y_test, pred)))